{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27eb4c22",
   "metadata": {},
   "source": [
    "# Notebook for generating training data for NeRF from ROSBAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load requirements for working with point clouds\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import tensorflow as tf\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 10*1024 #22*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "from ICET_spherical import ICET\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as p\n",
    "from nerf_utils import *\n",
    "from coarse_network_utils import*\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56594f51",
   "metadata": {},
   "source": [
    "# Download dataset from external source\n",
    "\n",
    "ex: https://ori-drs.github.io/newer-college-dataset/\n",
    "\n",
    "Move to corresponding folder in ../data/\n",
    "\n",
    "\n",
    "# Export point clouds from ROSBAG to csv file  \n",
    "\n",
    "1. edit <bag2mapframe.py> to save generated point clouds to correct directory\n",
    "\n",
    "\n",
    "2. Move <bag2mapframe.py> to ros directory on your machine and run it with:\n",
    "\n",
    "```\n",
    "mv bag2mapframe.py ~/catkin_ws/src/bagconverter\n",
    "cd ~/catkin_ws\n",
    "catkin_make\n",
    "cd src/bagconverter\n",
    "roscore\n",
    "python3 bag2mapframe.py\n",
    "```\n",
    "3. play rosbag \n",
    "\n",
    "```\n",
    "rosbag play -r 0.1 myBag.bag\n",
    "```\n",
    "\n",
    "# Load and filter ground truth pose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"~/PLINK/data/NewerCollegeDataset/\"\n",
    "experiment_name = \"01_short_experiment-20230331T172433Z-009/01_short_experiment/\"\n",
    "fn_gt = dir_name + experiment_name + \"ground_truth/registered_poses.csv\"\n",
    "#sec,nsec,x,y,z,qx,qy,qz,qw\n",
    "gt = np.loadtxt(fn_gt, delimiter=',',skiprows = 1)\n",
    "seconds = gt[:, 0]\n",
    "nano_seconds = gt[:, 1]\n",
    "xyz = gt[:, 2:5]\n",
    "qxyzw = gt[:, 5:]\n",
    "num_poses = qxyzw.shape[0]\n",
    "sensor_poses = np.eye(4, dtype=np.float64).reshape(1, 4, 4).repeat(num_poses, axis=0)\n",
    "sensor_poses[:, :3, :3] = R.from_quat(qxyzw).as_matrix()\n",
    "sensor_poses[:, :3, 3] = xyz\n",
    "T_CL = np.eye(4, dtype=np.float32)\n",
    "T_CL[:3, :3] = R.from_quat([0.0, 0.0, 0.924, 0.383]).as_matrix() #was this --1134.97 deg\n",
    "T_CL[:3, 3] = np.array([-0.084, -0.025, 0.050], dtype=np.float32) #was this\n",
    "sensor_poses = np.einsum(\"nij,jk->nik\", sensor_poses, T_CL)\n",
    "initial_pose = np.linalg.inv(sensor_poses[0]) \n",
    "poses_timestamps = seconds * 10e9 + nano_seconds\n",
    "sensor_poses = np.einsum(\"ij,njk->nik\", np.linalg.inv(sensor_poses[0]), sensor_poses) #TRY COMMENTING OUT...\n",
    "\n",
    "#get body frame vel to remove motion disortion from training data\n",
    "vel_world_frame = np.diff(sensor_poses[:,:3,-1], axis = 0)\n",
    "vel_body_frame = np.linalg.pinv(sensor_poses[1:,:3,:3]) @ vel_world_frame[:,:,None]\n",
    "vel_body_frame = vel_body_frame[:,:,0]\n",
    "#smooth out velocity estimates\n",
    "def moving_average(a, n=10):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "window=50\n",
    "MAx = moving_average(vel_body_frame[:,0], n = window)\n",
    "MAy = moving_average(vel_body_frame[:,1], n = window)\n",
    "MAz = moving_average(vel_body_frame[:,2], n = window)\n",
    "vel_body_frame = np.array([MAx, MAy, MAz]).T\n",
    "\n",
    "rot_vel_euls = np.diff(R.from_matrix(sensor_poses[:,:3,:3]).as_euler('xyz'), axis = 0)\n",
    "idx = np.argwhere(rot_vel_euls > (np.pi))\n",
    "rot_vel_euls[idx] = 0\n",
    "idx = np.argwhere(rot_vel_euls < (-np.pi))\n",
    "rot_vel_euls[idx] = 0\n",
    "\n",
    "#load HD map\n",
    "#courtyard\n",
    "pl = '~/PLINK/data/NewerCollegeDataset/new-college-29-01-2020-1cm-resolution-1stSection - mesh.ply'\n",
    "#forest\n",
    "# pl = '~/PLINK/data/NewerCollegeDataset/new-college-29-01-2020-1cm-resolution-5thSection.ply'\n",
    "HD_map = trimesh.load(pl).vertices\n",
    "show_nth = 5 #10\n",
    "submap = HD_map[::show_nth]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07167fa",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 24 #240 \n",
    "n_rots = 128 #128    #number of horizontal patches in 360 degrees\n",
    "n_vert_patches = 1 #1 #number of vertical patches between phimin and phimaxs\n",
    "useICET = True #need to turn off when working with the foliage dataset???\n",
    "image_width = 1024//n_rots\n",
    "image_height = 64//n_vert_patches\n",
    "shrink_factor = 0.005 #courtyard\n",
    "\n",
    "n_cols_to_skip = n_rots // 8 #remove this much from the beginning and end of each scan\n",
    "                             #   (need to remove parts of frame containing researcher carrying LIDAR)\n",
    "#Ouster OS1-64\n",
    "#took forever to calibrate this correctly-- not the same as on the sensor spec sheet!\n",
    "phimin = np.deg2rad(-15.594) \n",
    "phimax = np.deg2rad(17.743)\n",
    "vert_fov = np.rad2deg(phimax-phimin)\n",
    "\n",
    "poses = np.zeros([n_images*n_rots*n_vert_patches,4,4])\n",
    "images = np.ones([n_images*n_rots*n_vert_patches, 64//n_vert_patches, 1024//n_rots, 2]) #depth and raydrop channels\n",
    "# [n total \"patches\", patch height, patch width, xyz]\n",
    "rays_o_all = np.zeros([n_images*n_rots*n_vert_patches, 64//n_vert_patches, 1024//n_rots, 3]) \n",
    "rays_d_all = np.zeros([n_images*n_rots*n_vert_patches, 64//n_vert_patches, 1024//n_rots, 3]) \n",
    "\n",
    "H, W = images.shape[1:3]\n",
    "redfix_hist = np.zeros([n_images,4,4]) #holds on to the corrective transforms we get from ICET \n",
    "\n",
    "for i in range(n_images):\n",
    "    print(i) \n",
    "    #full loop first courtyard\n",
    "    idx = i*50 + 7650 #for debug and visualization\n",
    "#     idx = i*5 + 7650 #train set\n",
    "#     idx = i*40 + 10600 #forest\n",
    "    fn1 = \"~/PLINK/data/NewerCollegeDataset/01_Short_Experiment/point_clouds/frame_\" + str(idx) + \".npy\"\n",
    "    pc1 = np.load(fn1)\n",
    "\n",
    "    #apply distortion correction ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #m_hat = [dx, dy, dz, droll, dpitch, dyaw]\n",
    "    m_hat = np.array([-vel_body_frame[idx,0],\n",
    "                      -vel_body_frame[idx,1],\n",
    "                      -vel_body_frame[idx,2],\n",
    "#                       -rot_vel_euls[idx,0], \n",
    "#                       -rot_vel_euls[idx,1],\n",
    "#                       -rot_vel_euls[idx,2]\n",
    "                      0.,0.,0.\n",
    "                     ])   \n",
    "    pc1 = apply_motion_profile(pc1, m_hat, period_lidar=1.)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    pc1 = np.flip(pc1, axis = 0)\n",
    "\n",
    "    #Register undistorted PC against HD Map using ICET to correct issues in ground truth\n",
    "    if useICET:\n",
    "        submap_in_pc1_frame = (np.linalg.pinv(sensor_poses[idx]) @ initial_pose @ np.append(submap, np.ones([len(submap),1]), axis =1).T).T #test\n",
    "        submap_in_pc1_frame = submap_in_pc1_frame[:,:3]\n",
    "\n",
    "        initial_guess = tf.constant([0.,0.,0.,0.,0.,0.])\n",
    "        it = ICET(cloud1 = submap_in_pc1_frame, cloud2 = pc1, fid = 50, niter = 8, \n",
    "           draw = False, group = 2, RM = False, DNN_filter = False, x0 = initial_guess)\n",
    "\n",
    "        pc1_in_map_frame = (initial_pose @ sensor_poses[idx] @ np.append(pc1, np.ones([len(pc1),1]), axis =1).T).T #test\n",
    "        pc1_in_map_frame = pc1_in_map_frame[:,:3]\n",
    "\n",
    "        pc1_corrected_in_map_frame = (initial_pose @ sensor_poses[idx] @ np.append(it.cloud2_tensor.numpy(), np.ones([len(it.cloud2_tensor.numpy()),1]), axis =1).T).T #test\n",
    "        pc1_corrected_in_map_frame = pc1_corrected_in_map_frame[:,:3]    \n",
    "\n",
    "        #draw red scan corrected by output of ICET\n",
    "        redFix = np.eye(4)\n",
    "        redFix[:3,-1] = it.X[:3]\n",
    "        redFix[:3,:3] = redFix[:3,:3] @ R.from_euler('xyz', [it.X[3], it.X[4], it.X[5]]).as_matrix()\n",
    "        redfix_hist[i] = redFix\n",
    "        redScanFixed = (redFix @ np.append(pc1, np.ones([len(pc1),1]), axis =1).T).T\n",
    "        redScanFixed = (sensor_poses[idx] @ np.append(redScanFixed[:,:3], np.ones([len(redScanFixed),1]), axis =1).T).T\n",
    " \n",
    "    else:\n",
    "        redFix = np.eye(4)\n",
    "        redfix_hist[i] = redFix\n",
    "        redScanFixed = (redFix @ np.append(pc1, np.ones([len(pc1),1]), axis =1).T).T\n",
    "        redScanFixed = (sensor_poses[idx] @ np.append(redScanFixed[:,:3], np.ones([len(redScanFixed),1]), axis =1).T).T\n",
    "\n",
    "    #convert to depth image\n",
    "    pc1_spherical = cartesian_to_spherical(pc1).numpy() #[r, theta, phi]\n",
    "    pcs = np.reshape(pc1_spherical, [-1,64,3])\n",
    "    pcs = np.flip(pcs, axis = 1)\n",
    "    raw_data = pcs[:,:,:]\n",
    "    raw_data = np.transpose(pcs, [1,0,2])\n",
    "\n",
    "    #destagger depth images (OS1 unit has delay in sensor return bus)\n",
    "    data = np.zeros([64, 1024])\n",
    "    for k in range(np.shape(data)[0]//4):\n",
    "        data[4*k,1:-8] = raw_data[4*k,9:,0]\n",
    "        data[4*k+1,1:-2] = raw_data[4*k+1,3:,0]\n",
    "        data[4*k+2,4:] = raw_data[4*k+2,:-4,0]\n",
    "        data[4*k+3,10:] = raw_data[4*k+3,:-10,0]\n",
    "    data = np.flip(data, axis =1)\n",
    "\n",
    "    #get rays_o and rays_d directly inside data generation loop \n",
    "    rotm = sensor_poses[idx] @ redfix_hist[i]\n",
    "    rotm[0,-1] += 30\n",
    "    rotm[1,-1] += 30\n",
    "    rotm[2,-1] += 15 \n",
    "    rotm[:3,-1] *= shrink_factor #0.02 #0.005 #COURTYARD\n",
    "    #courtyard\n",
    "    rotm[0,-1] += 0.01 \n",
    "    rotm[1,-1] += 0.25 \n",
    "    rotm[2,-1] += 0.25 #translate above xy plane\n",
    "#     #forest\n",
    "#     rotm[0,-1] += 1.2 \n",
    "#     rotm[1,-1] += 1.25 \n",
    "#     rotm[2,-1] += 0.25 #translate above xy plane\n",
    "    ro, rd = get_rays_from_point_cloud(pc1, m_hat, rotm) \n",
    "\n",
    "    for j in range(n_rots):\n",
    "        for k in range(n_vert_patches):    \n",
    "            #store rays_o and rays_d info\n",
    "            rd_in_patch = rd[k*image_height:(k+1)*image_height,j*image_width:(j+1)*image_width, :]\n",
    "            rays_d_all[k+(j+(i*n_rots))*n_vert_patches,:,:,:] = rd_in_patch\n",
    "            ro_in_patch = ro[k*image_height:(k+1)*image_height,j*image_width:(j+1)*image_width, :]\n",
    "            rays_o_all[k+(j+(i*n_rots))*n_vert_patches,:,:,:] = ro_in_patch            \n",
    "\n",
    "            #get cropped depth image ~~~~~~~~~~~~~~~~~~~~    \n",
    "            #crop vertically and horizontally\n",
    "            pcs = data[k*image_height:(k+1)*image_height,j*image_width:(j+1)*image_width] \n",
    "            #save depth information to first channel\n",
    "            images[k+(j+(i*n_rots))*n_vert_patches,:,:,0] = pcs\n",
    "            #save raydrop mask to 2nd channel\n",
    "            a = np.argwhere(abs(pcs) < 1)\n",
    "            images[k+(j+(i*n_rots))*n_vert_patches, a[:,0],a[:,1],1] = 0\n",
    "\n",
    "            #get transformation matrix ~~~~~~~~~~~~~~~~~~\n",
    "            #centers origin at actual origin of HD map \n",
    "            rotm = sensor_poses[idx] @ redfix_hist[i]\n",
    "\n",
    "            crop_angle = j*(2*np.pi/n_rots) - np.pi/2 + (np.pi/n_rots)\n",
    "            #account for the fact that sensor points back and to the left\n",
    "            rotm_crop = R.from_euler('xyz', [0,0,-crop_angle]).as_matrix() #test\n",
    "            rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "            rotm[0,-1] += 30\n",
    "            rotm[1,-1] += 30\n",
    "            rotm[2,-1] += 15 \n",
    "            rotm[:3,-1] *= shrink_factor\n",
    "            images[k+(j+(i*n_rots))*n_vert_patches,:,:,0] *= shrink_factor\n",
    "            #courtyard\n",
    "            rotm[0,-1] += 0.01 #shift up just a little\n",
    "            rotm[1,-1] += 0.25 #shift towards positive x\n",
    "            rotm[2,-1] += 0.25 #translate above xy plane\n",
    "#             #forest\n",
    "#             rotm[0,-1] += 1.2 \n",
    "#             rotm[1,-1] += 1.25 \n",
    "#             rotm[2,-1] += 0.25 #translate above xy plane\n",
    "\n",
    "            poses[k+(j+(i*n_rots))*n_vert_patches] = rotm \n",
    "\n",
    "# # Remove patches where sensor is occluded by person holding lidar \n",
    "#calculate how many columns of patches we need to skip at the beginning and end of each scan to avoid\n",
    "bad_idx = np.zeros([0,n_rots - 2*n_cols_to_skip])\n",
    "a = np.linspace(0,n_rots*n_images*n_vert_patches-1,n_rots*n_images*n_vert_patches)\n",
    "for i in range(n_vert_patches*n_cols_to_skip):\n",
    "    bad_i_left = a[i::n_rots*n_vert_patches]\n",
    "    bad_idx = np.append(bad_idx, bad_i_left)\n",
    "    bad_i_right = a[(i+n_vert_patches*(n_rots-n_cols_to_skip))::n_rots*n_vert_patches]\n",
    "    bad_idx = np.append(bad_idx, bad_i_right)\n",
    "\n",
    "bad_idx = np.sort(bad_idx)\n",
    "all_idx = np.linspace(0,n_rots*n_images*n_vert_patches-1,n_rots*n_images*n_vert_patches)\n",
    "good_idx = np.setdiff1d(all_idx, bad_idx).astype(int)\n",
    "images = images[good_idx,:,:,:]\n",
    "poses = poses[good_idx,:,:]\n",
    "rays_d_all = rays_d_all[good_idx,:,:,:]\n",
    "rays_o_all = rays_o_all[good_idx,:,:,:]\n",
    "\n",
    "images = images.astype(np.float32)\n",
    "poses = poses.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a3982",
   "metadata": {},
   "source": [
    "# Visualize training data\n",
    "\n",
    "draw training data in the same frame using depth images, ray origins (rays_o), \n",
    "and view directions (rays_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_frame_from_rays(disp, n_rots=128, n_vert_patches=1, frameIdx=0, \n",
    "                         color = 'red', stitched_map = np.zeros([0,3]) ):\n",
    "\n",
    "    phimin = np.deg2rad(-15.594) #took forever to figure this out...\n",
    "    phimax = np.deg2rad(17.743)\n",
    "    H = 64 // n_vert_patches\n",
    "    W = 1024 // n_rots\n",
    "    vertical_bins = np.linspace(phimin, phimax, n_vert_patches+1)  \n",
    "    phivals = np.linspace(phimin, phimax, 64)#new (correct) way to bin elevation angles\n",
    "    n_cols_to_skip = n_rots // 8\n",
    "\n",
    "    pts1 = np.zeros([1,3])\n",
    "    for p in range(frameIdx*(n_rots - 2*n_cols_to_skip), (frameIdx + 1 )*(n_rots - 2*n_cols_to_skip)):\n",
    "        for i in range(n_vert_patches):\n",
    "            img_i = i\n",
    "            idx_first=len(phivals) - (img_i%(n_vert_patches))*(64//n_vert_patches)-1\n",
    "            idx_second= (len(phivals)- ((img_i+1)%(n_vert_patches))*(64//n_vert_patches))%len(phivals)\n",
    "            phimin_patch = phivals[idx_first]\n",
    "            phimax_patch = phivals[idx_second]\n",
    "\n",
    "            pose = poses[i + p*n_vert_patches]\n",
    "            rays_o = rays_o_all[i + p*n_vert_patches]\n",
    "            rays_d = rays_d_all[i + p*n_vert_patches]\n",
    "            \n",
    "            inMap1 = add_patch(rays_o, rays_d, images[i+p*n_vert_patches,:,:,0])\n",
    "\n",
    "            pts1 = np.append(pts1, inMap1, axis = 0)\n",
    "        disp.append(Points(rays_o[0,:1,:], r = 15, c = 'purple')) #DEBUG \n",
    "\n",
    "    vizPts1 = Points(pts1, c = color, r = 3., alpha = 0.125)\n",
    "    disp.append(vizPts1)\n",
    "    \n",
    "    stitched_map = np.append(stitched_map, pts1, axis = 0)\n",
    "    return stitched_map\n",
    "    \n",
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]          \n",
    "\n",
    "stitched_map = np.zeros([0,3])\n",
    "# colors = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'red']\n",
    "colors = np.linspace(0.1,0.3,n_images)[:,None] * np.array([[1,1,1]])\n",
    "for i in range(len(colors)):\n",
    "    print(i)\n",
    "    stitched_map = draw_frame_from_rays(disp, n_rots = 128, n_vert_patches=1, \n",
    "                                        frameIdx = i, color=colors[i], stitched_map=stitched_map)\n",
    "    \n",
    "plt.show(disp, \"Drawing training data from depth images, rays_o, and rays_d\")\n",
    "ViewInteractiveWidget(plt.window)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef543b4b",
   "metadata": {},
   "source": [
    "# Save training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0277053",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"~/PLINK/data/NewerCollegeDataset/images.npy\", images)\n",
    "np.save(\"~/PLINK/data/NewerCollegeDataset/poses.npy\", poses)\n",
    "np.save(\"~/PLINK/data/NewerCollegeDataset/rays_o.npy\", rays_o_all)\n",
    "np.save(\"~/PLINK/data/NewerCollegeDataset/rays_d.npy\", rays_d_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
