{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9add2626",
   "metadata": {},
   "source": [
    "# Notebook for training PLiNK on Newer College Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load requirements for working with PCs\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import tensorflow as tf\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 10*1024 #22*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as p\n",
    "from nerf_utils import *\n",
    "from coarse_network_utils import*\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91339db6",
   "metadata": {},
   "source": [
    "### load pre-processed training data \n",
    "\n",
    "see notebook \"generate_training_data.ipynb\" for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6933ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full loop of Courtyard\n",
    "images = np.load(\"~/PLINK/data/NewerCollegeDataset/images.npy\")\n",
    "poses = np.load(\"~/PLINK/data/NewerCollegeDataset/poses.npy\")\n",
    "rays_o_all = np.load(\"~/PLINK/data/NewerCollegeDataset/rays_o.npy\")\n",
    "rays_d_all = np.load(\"~/PLINK/data/NewerCollegeDataset/rays_d.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830bfc5",
   "metadata": {},
   "source": [
    "## Train fine network first for a warm start before jointly training two networks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd2ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "optimizer = tf.keras.optimizers.Adam(5e-4) #default tiny-NeRF\n",
    "\n",
    "N_samples = 128 #128 #256\n",
    "near=0.\n",
    "far= 1.\n",
    "N_iters = 5_000_000\n",
    "psnrs = []\n",
    "iternums = []\n",
    "i_plot = 128\n",
    "accumulate_gradients_steps = 1 #32\n",
    "\n",
    "n_rots = 128 #128 #number of horizontal patches per 2*pi\n",
    "n_vert_patches = 1 #number of vertical patches between phimin and phimax (at time of data generation)\n",
    "H = 64 // n_vert_patches\n",
    "W = 1024 // n_rots\n",
    "testimg = images[12]\n",
    "testpose = poses[12]\n",
    "\n",
    "#actual upper and lower elevation limits of LiDAR sensor -- ignore the values in the Ouster OS1 Spec sheet!!\n",
    "phimin = np.deg2rad(-15.594) #observed in raw data \n",
    "phimax = np.deg2rad(17.743) #observed in raw data\n",
    "\n",
    "vertical_bins = np.linspace(phimin, phimax, n_vert_patches+1)  \n",
    "phivals = np.linspace(phimin, phimax, 64)\n",
    "gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "accumulated_loss = 0.0\n",
    "\n",
    "for i in range(N_iters+1):\n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "\n",
    "    target = images[img_i,:,:,:1]\n",
    "    target_drop_mask = images[img_i,:,:,1:]\n",
    "    pose = poses[img_i]\n",
    "    rays_d = rays_d_all[img_i]\n",
    "    rays_o = rays_o_all[img_i]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # run coarse network~~~~~~~~~~~~~~~~~\n",
    "        z_vals = tf.linspace(near, far, N_samples)  #z_vals must be in ascending order \n",
    "        z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "        z_vals = z_vals[:,:,:,None] #manually expand dimensions before passing in to coarse network (all pixels will share the same z_vals)\n",
    "        depth, ray_drop, CDF, weights = render_rays(model, rays_o, rays_d,  z_vals)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "        gtCDF = z_vals[:,:,:,0] > target[:,:,:]\n",
    "        gtCDF = tf.cast(gtCDF, tf.float32)\n",
    "\n",
    "        loss = calculate_loss(depth, ray_drop, target, target_drop_mask, CDF = CDF, gtCDF = gtCDF)\n",
    "\n",
    "        #prevent NaN gradients from crashing training routine\n",
    "        current_gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        current_gradients = [grad if grad is not None else tf.zeros_like(var) for grad, var in zip(current_gradients, model.trainable_variables)]\n",
    "        gradients = [grad_accum + current_grad for grad_accum, current_grad in zip(gradients, current_gradients)]        \n",
    "        accumulated_loss += loss\n",
    "    \n",
    "    if i%accumulate_gradients_steps==0:    \n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        accumulated_loss = 0.0\n",
    "        gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "        accumulated_loss = 0\n",
    "    \n",
    "    if i%i_plot==0:\n",
    "        rays_o, rays_d = get_rays(H, W, testpose, vertical_bins[-2], vertical_bins[-1]) #constant validation image\n",
    "        z_vals = tf.linspace(near, far, N_samples) \n",
    "        z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "        z_vals = z_vals[:,:,:,None]\n",
    "        depth, ray_drop, CDF, weights = render_rays(model, rays_o, rays_d,  z_vals)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "        target = testimg[:,:,:1]\n",
    "        target_drop_mask = testimg[:,:,1:]\n",
    "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
    "        psnrs.append(psnr.numpy())\n",
    "        iternums.append(i)\n",
    "        p.figure(figsize=(10,4))\n",
    "        p.subplot(131)\n",
    "        p.imshow(depth,cmap = \"gray\")#, norm='log')\n",
    "        p.title(f'Estimated Depth at Iteration: {i}')\n",
    "        p.subplot(133)\n",
    "        p.plot(iternums, psnrs)\n",
    "        p.title('PSNR')\n",
    "        p.subplot(132)\n",
    "        p.imshow(ray_drop, cmap=\"gray\")#, norm = 'log')\n",
    "        p.title(\"estimated ray drop mask\")\n",
    "        p.show()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ebddb7",
   "metadata": {},
   "source": [
    "#### Reproduce first training scan using fine network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True)\n",
    "disp=[]\n",
    "\n",
    "N_samples = 128\n",
    "n_cols_to_skip = n_rots // 8 #mimic trainig data where researcher blocks large portion of frame\n",
    "for j in range(n_rots * n_vert_patches - 2*n_cols_to_skip*n_vert_patches):\n",
    "    idx = j + 20*(n_rots - 2*n_cols_to_skip)\n",
    "    \n",
    "    pose = poses[idx]\n",
    "    rays_d = rays_d_all[idx]\n",
    "    rays_o = rays_o_all[idx]\n",
    "    \n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "    z_vals += 1.0*tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "    z_vals = z_vals[:,:,:,None]\n",
    "    \n",
    "    depth, ray_drop, CDF, weights = render_rays(model, rays_o, rays_d,  z_vals)\n",
    "    \n",
    "    new_point_cloud_spherical = np.zeros([np.shape(depth)[0]*np.shape(depth)[1],3])\n",
    "    depth = tf.transpose(depth).numpy()\n",
    "    depth = np.flip(depth, axis = 0)\n",
    "    \n",
    "    #scale back up to normal size (positional encoding requires all points scaled [0,1])\n",
    "    depth *= 200 \n",
    "    ray_drop = tf.transpose(ray_drop).numpy()\n",
    "    ray_drop = np.flip(ray_drop, axis = 0)\n",
    "    \n",
    "    count = 0\n",
    "    for w in range(W):\n",
    "        for h in range(H):\n",
    "            #draw all points\n",
    "#             new_point_cloud_spherical[count,0] = depth[w,h] #radius \n",
    "            # suppress ray dropped points\n",
    "            if ray_drop[w,h] > 0.9:             \n",
    "                    new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "            else:\n",
    "                    new_point_cloud_spherical[count,0] = 0 \n",
    "            new_point_cloud_spherical[count,1] = (w-(1024//(2*n_rots)))/(2048//(2*n_rots))*(2*np.pi/n_rots)\n",
    "            new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/(H - 1))  \n",
    "            count+= 1\n",
    "\n",
    "    new_point_cloud_spherical[:,1] -= (np.pi/n_rots) + j*(2*np.pi/n_rots) + np.pi\n",
    "    new_point_cloud_spherical[:,2] -= (phimax+phimin)\n",
    "    \n",
    "    new_point_cloud_cart = spherical_to_cartesian(new_point_cloud_spherical).numpy() \n",
    "    new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2] #need to flip z for visualization\n",
    "\n",
    "    disp.append(Points(new_point_cloud_cart, c = 'gray', r = 3, alpha = 0.5))\n",
    "    \n",
    "plt.show(disp, \"Reproducing first training scan using single network\")\n",
    "ViewInteractiveWidget(plt.window)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b48c0c",
   "metadata": {},
   "source": [
    "# Train Dual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nerf_utils import *\n",
    "\n",
    "model_coarse = init_model_proposal()\n",
    "model_fine = init_model()\n",
    "model_fine = model #warm start from previous training loop\n",
    "loss_hist = np.zeros([0])\n",
    "\n",
    "#1e-4 -> 1e-6\n",
    "optimizer_coarse = tf.keras.optimizers.Adam(1e-4)\n",
    "#1e-4 -> 1e-6\n",
    "optimizer_fine = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "n_bins_fine = 128 #512 \n",
    "n_bins_coarse = 16 #64\n",
    "near=0.\n",
    "far=1.\n",
    "N_iters = 10_000\n",
    "psnrs = []\n",
    "iternums = []\n",
    "i_plot = 128 #256\n",
    "\n",
    "n_rots = 128 #128 #number of horizontal patches per 2*pi\n",
    "n_vert_patches = 1 #8 #number of vertical patches between phimin and phimax\n",
    "H = 64 // n_vert_patches\n",
    "W = 1024 // n_rots\n",
    "vertical_bins = np.linspace(phimin, phimax, n_vert_patches+1)  \n",
    "phivals = np.linspace(phimin, phimax, 64)#new (correct) way to bin elevation angles\n",
    "\n",
    "loss_since_last_plot = np.zeros([0])\n",
    "for i in range(N_iters):\n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "\n",
    "    #use full vertical span of patch\n",
    "    target = images[img_i,:,:,:1]\n",
    "    target_drop_mask = images[img_i,:,:,1:]\n",
    "    pose = poses[img_i]\n",
    "    rays_d = rays_d_all[img_i]\n",
    "    rays_o = rays_o_all[img_i]\n",
    "        \n",
    "#     # break up scans vertically at train time -- do this as needed to prevent OOM errors\n",
    "#     # make sure to drop the LR proportionally!\n",
    "#     patch_height = 16\n",
    "#     vert_crop_start = np.random.randint(0,H - patch_height - 1)\n",
    "#     vert_crop_end = vert_crop_start + patch_height\n",
    "#     target = images[img_i,vert_crop_start:vert_crop_end,:,:1]\n",
    "#     target_drop_mask = images[img_i,vert_crop_start:vert_crop_end,:,1:]\n",
    "#     pose = poses[img_i]\n",
    "#     rays_d = rays_d_all[img_i, vert_crop_start:vert_crop_end]\n",
    "#     rays_o = rays_o_all[img_i, vert_crop_start:vert_crop_end]\n",
    "    \n",
    "    idx_first=len(phivals) - (img_i%(n_vert_patches))*(64//n_vert_patches)-1\n",
    "    idx_second= (len(phivals)- ((img_i+1)%(n_vert_patches))*(64//n_vert_patches))%len(phivals)\n",
    "    phimin_patch = phivals[idx_first]\n",
    "    phimax_patch = phivals[idx_second]\n",
    "\n",
    "    with tf.GradientTape() as tape_coarse, tf.GradientTape() as tape_fine:\n",
    "        #run coarse network first to get locations to evaluate fine model at ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        z_vals_coarse = tf.linspace(near, far - (far/n_bins_coarse), n_bins_coarse)  #IMPORTANT NOTE: z_vals must be in ascending order \n",
    "        z_vals_coarse += 0.0*tf.random.uniform(list(rays_o.shape[:-1]) + [n_bins_coarse]) * (far-near)/n_bins_coarse\n",
    "        z_vals_coarse = z_vals_coarse[:,:,:,None]#manually expand dimensions before passing in to coarse network (all pixels will share the same z_vals)\n",
    "        #move z vals coarse to center of histogram bins \n",
    "        width_coarse = tf.experimental.numpy.diff(z_vals_coarse, axis=2)\n",
    "        width_coarse = tf.concat([width_coarse, 1.- z_vals_coarse[:,:,-1][:,:,None] ], axis=2)    \n",
    "        z_vals_coarse = z_vals_coarse + width_coarse/2\n",
    "\n",
    "        z_vals_fine, width_fine, weights_coarse = run_coarse_network(model_coarse, z_vals_coarse, width_coarse, \n",
    "                                                                     rays_o, rays_d, n_resample = n_bins_fine)\n",
    "        z_vals_fine = z_vals_fine[:, :, :, None]\n",
    "        weights_coarse = weights_coarse[:, :, :, None]\n",
    "                \n",
    "        # run fine network to get actual scene density ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~        \n",
    "        depth, ray_drop, CDF, weights_fine = render_rays(model_fine, rays_o, rays_d,  z_vals_fine)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "        gtCDF = z_vals_fine[:,:,:,0] > target[:,:,:]\n",
    "        gtCDF = tf.cast(gtCDF, tf.float32)\n",
    "        \n",
    "        #weights of zero will produce zero area, leading to NaN loss        \n",
    "        epsilon = 1e-6\n",
    "        padded_area_by_bin = (weights_fine + epsilon*tf.ones_like(width_fine)) * width_fine #slightly closer?\n",
    "        area_fine = tf.reduce_sum(padded_area_by_bin, axis=2, keepdims=True)       \n",
    "        weights_fine /= area_fine\n",
    "\n",
    "        #per advice of mip-nerf 360, calculate fine losss first then apply gradient stop\n",
    "        # so that the fine network doesn't dumb itslef down to appease coarse loss function\n",
    "        loss_fine = calculate_loss(depth, ray_drop, target, target_drop_mask, CDF = CDF, gtCDF = gtCDF)\n",
    "        weights_fine_stopped = tf.stop_gradient(weights_fine)\n",
    "        loss_coarse, fine_sum = calculate_loss_coarse_network(z_vals_coarse[:,:,:,0], \n",
    "                                                                z_vals_fine[:,:,:,0], \n",
    "                                                                weights_coarse[:,:,:,0], \n",
    "                                                                weights_fine_stopped, \n",
    "                                                                width_coarse[:,:,:,0],\n",
    "                                                                width_fine[:,:,:],\n",
    "                                                                debug = True)\n",
    "        loss_coarse = tf.math.reduce_sum(loss_coarse)\n",
    "        gradients_fine = tape_fine.gradient(loss_fine, model_fine.trainable_variables)\n",
    "        optimizer_fine.apply_gradients(zip(gradients_fine, model_fine.trainable_variables))        \n",
    "        gradients_coarse = tape_coarse.gradient(loss_coarse, model_coarse.trainable_variables)\n",
    "        optimizer_coarse.apply_gradients(zip(gradients_coarse, model_coarse.trainable_variables))\n",
    "\n",
    "\n",
    "    if i % i_plot == 0:\n",
    "        loss_hist = np.append(loss_hist, np.mean(loss_since_last_plot))\n",
    "        loss_since_last_plot = np.zeros([0])\n",
    "        \n",
    "        #rescale weights_coarse before plotting\n",
    "        eps = 1e-3\n",
    "        weights_coarse = weights_coarse + eps*tf.ones_like(weights_coarse)\n",
    "        weights_coarse = weights_coarse/ tf.math.reduce_sum(width_coarse*weights_coarse, axis = 2)[:,:,None]\n",
    "        mask = tf.cast(fine_sum > weights_coarse[:,:,:,0], tf.float32)\n",
    "        L_along_ray = mask * (fine_sum - weights_coarse[:,:,:,0]) * width_coarse[:,:,:,0]\n",
    "\n",
    "        look_at = 0\n",
    "        p.figure(figsize=(10,4))\n",
    "        p.subplot(141)\n",
    "        p.imshow(depth,cmap = \"gray\")#, norm='log')\n",
    "        p.title(f'Estimated Depth at Iteration: {i}')\n",
    "        p.subplot(143)\n",
    "        p.bar(z_vals_coarse[look_at,0,:,0], weights_coarse[look_at,0,:,0], width=width_coarse[look_at,0,:,0],\n",
    "             alpha = 0.4, label = 'coarse network output', hatch = '\\\\', edgecolor = 'black')\n",
    "        p.bar(z_vals_coarse[look_at,0,:,0], fine_sum[look_at,0,:], width=width_coarse[look_at,0,:,0],\n",
    "             alpha = 0.4, label = 'fine sum', hatch = '//', edgecolor = 'black')\n",
    "        p.bar(z_vals_fine[look_at,0,:,0], weights_fine[look_at,0,:], width=width_fine[look_at,0,:],\n",
    "             alpha = 0.4, label= 'fine network output')\n",
    "        p.bar(z_vals_coarse[look_at,0,:,0], L_along_ray[look_at,0,:]/width_coarse[look_at,0,:,0], width=width_coarse[look_at,0,:,0],\n",
    "              bottom = weights_coarse[look_at,0,:,0], alpha = 0.4, color = 'red', \n",
    "              label= 'loss of coarse')    \n",
    "        p.ylim([0,50])\n",
    "        p.title('coarse network weights for ray')\n",
    "        p.legend(loc=\"best\")\n",
    "        p.subplot(142)\n",
    "        p.imshow(ray_drop, cmap=\"gray\")#, norm = 'log')\n",
    "        p.title(\"estimated ray drop mask\")\n",
    "        p.subplot(144)\n",
    "        p.title(\"loss history\")\n",
    "        p.plot(loss_hist)\n",
    "        p.show()\n",
    "    else:\n",
    "        loss_since_last_plot = np.append(loss_since_last_plot, loss_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce766b",
   "metadata": {},
   "source": [
    "# Use the two networks to generate a point cloud at a novel frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7978e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_view = 64 #128 #number of (rotational?) patches to draw\n",
    "n_bins_coarse = 32 #32\n",
    "n_bins_fine = 128 #1024 #2048 #512 #256\n",
    "\n",
    "start_frame_idx = 100 #320 #100 #20\n",
    "start_idx = start_frame_idx*(128 - 2*n_cols_to_skip)\n",
    "\n",
    "near=0.\n",
    "far= 1.\n",
    "n_rots = 64 #128 #number of horizontal patches per 2*pi\n",
    "n_vert_patches = 1 #number of vertical patches between phimin and phimax\n",
    "\n",
    "H = 64 // n_vert_patches\n",
    "W = 1024 // n_rots\n",
    "# phimin = np.deg2rad(-15.593) #observed in raw data\n",
    "# phimax = np.deg2rad(17.743)\n",
    "phimax = np.deg2rad(15.593) #test -- might actually look better at render time?\n",
    "phimin = np.deg2rad(-17.743)\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "savepc = np.zeros([0,3]) #to save point cloud to external file\n",
    "\n",
    "very_beginning = time.time()\n",
    "\n",
    "for j in range(num_view):\n",
    "    print(j,\"/\",num_view)\n",
    "\n",
    "    #get sensor transformation matrix\n",
    "    rotm = poses[start_idx].copy()\n",
    "\n",
    "    #center in courtyard dataset\n",
    "    rotm[:3,-1] = np.array([5., 55., -13.])/200\n",
    "    #center in forest dataset -- startidx = 320\n",
    "    #rotm[:3,-1] += np.array([25., 5, 0.])/200\n",
    "\n",
    "    # account for image crop in rotation\n",
    "    crop_angle =  -(3*np.pi/n_rots) - j*(2*np.pi/n_rots) #test\n",
    "    rotm_crop = R.from_euler('xyz', [0,0,-crop_angle + 3*np.pi/4]).as_matrix() \n",
    "    rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "    rotm = rotm.astype(np.float32)\n",
    "    vertical_bins = np.linspace(phimin, phimax, n_vert_patches+1)\n",
    "    phimin_patch = vertical_bins[img_i%n_vert_patches] \n",
    "    phimax_patch = vertical_bins[img_i%n_vert_patches + 1]\n",
    "    \n",
    "    #RUN COARSE NETWORK\n",
    "    rays_o, rays_d = get_rays(H, W, rotm, phimin_patch, phimax_patch)\n",
    "    z_vals_coarse = tf.linspace(near, far - (far/n_bins_coarse), n_bins_coarse)  #IMPORTANT NOTE: z_vals must be in ascending order \n",
    "    z_vals_coarse += 0.00*tf.random.uniform(list(rays_o.shape[:-1]) + [n_bins_coarse]) * (far-near)/n_bins_coarse\n",
    "    z_vals_coarse = z_vals_coarse[:,:,:,None]#manually expand dimensions before passing in to coarse network (all pixels will share the same z_vals)\n",
    "    #move z vals coarse to center of histogram bins \n",
    "    width_coarse = tf.experimental.numpy.diff(z_vals_coarse, axis=2)\n",
    "    width_coarse = tf.concat([width_coarse, 1.- z_vals_coarse[:,:,-1][:,:,None] ], axis=2)    \n",
    "    z_vals_coarse = z_vals_coarse + width_coarse/2\n",
    "    z_vals_fine, width_fine, weights_coarse = run_coarse_network(model_coarse, z_vals_coarse, width_coarse, \n",
    "                                                                 rays_o, rays_d, n_resample = n_bins_fine)   \n",
    "    eps = 1e-3\n",
    "    weights_coarse_scaled = weights_coarse + eps*tf.ones_like(weights_coarse)\n",
    "    weights_coarse_scaled = weights_coarse_scaled[:,:,:,None]/ tf.math.reduce_sum(width_coarse*weights_coarse_scaled[:,:,:,None], axis = 2)[:,:,None]\n",
    "    z_vals_fine = z_vals_fine[:, :, :, None]\n",
    "    weights_coarse = weights_coarse[:, :, :, None]\n",
    "        \n",
    "    #RUN FINE NETWORK\n",
    "    depth, ray_drop, CDF, weights_fine = render_rays(model_fine, rays_o, rays_d,  z_vals_fine, \n",
    "                                                     roll_override = 0.1)    \n",
    "    \n",
    "    #CONVERT OUTPUT TO POINT CLOUD IN CORRECT FRAME\n",
    "    new_point_cloud_spherical = np.zeros([np.shape(depth)[0]*np.shape(depth)[1],3])\n",
    "    depth = tf.transpose(depth).numpy()\n",
    "    depth = np.flip(depth, axis = 0)\n",
    "    #scale back up to true size\n",
    "    depth *= 200\n",
    "    ray_drop = tf.transpose(ray_drop).numpy()\n",
    "    ray_drop = np.flip(ray_drop, axis = 0)\n",
    "    count = 0\n",
    "    for w in range(W):\n",
    "        for h in range(H):\n",
    "#             new_point_cloud_spherical[count,0] = depth[w,h] #radius #draw all points\n",
    "            if ray_drop[w,h] > 0.95:             \n",
    "                    new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "            else:\n",
    "                    new_point_cloud_spherical[count,0] = 0#100 # suppress ray dropped points\n",
    "            new_point_cloud_spherical[count,1] = (w+(1024//(2*n_rots)))/(2048//(2*n_rots))*(2*np.pi/n_rots)   #was this\n",
    "            new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/(H - 1)) #[17.74,-15.59] #(correct)       \n",
    "            count+= 1\n",
    "    new_point_cloud_spherical[:,1] -= -j*(2*np.pi/n_rots)\n",
    "    new_point_cloud_spherical[:,2] -= (phimax+phimin)    \n",
    "    new_point_cloud_cart = spherical_to_cartesian(new_point_cloud_spherical).numpy()\n",
    "    new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2]\n",
    "    savepc = np.append(savepc, new_point_cloud_cart, axis = 0)\n",
    "    disp.append(Points(new_point_cloud_cart, c = 'gray', r = 3, alpha = 0.5))\n",
    "\n",
    "plt.show(disp, \"Coarse to Fine PLiNK Rendering\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757919e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
